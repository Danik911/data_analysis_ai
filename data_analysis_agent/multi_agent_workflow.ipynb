{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97370ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install llama-index llama-index-embeddings-openai qdrant-client llama-index-vector-stores-qdrant llama-index llama-index-llms-openai llama-index-vector-stores-faiss faiss-cpu llama-index-llms-anthropic tavily-python llama-index-experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d72ed41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nest_asyncio\n",
    "from getpass import getpass\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Import LlamaIndex components\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.agent import FunctionCallingAgent\n",
    "from llama_index.core.workflow import Event, Workflow, Context, StopEvent, step\n",
    "from llama_index.core.workflow import StartEvent\n",
    "import pandas as pd\n",
    "import re\n",
    "import functools\n",
    "from llama_index.experimental.query_engine import PandasQueryEngine\n",
    "from llama_index.core.workflow import Context\n",
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "# Load environment variables and apply nest_asyncio for async operations\n",
    "load_dotenv()\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Get API keys\n",
    "OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY') or getpass(\"Enter OPENAI_API_KEY: \")\n",
    "\n",
    "# Initialize LLM\n",
    "llm = OpenAI(model=\"gpt-4o-2024-11-20\", api_key=OPENAI_API_KEY, temperature=0.5, max_tokens=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "593b2b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def execute_pandas_query_tool(ctx: Context, query_str: str) -> str:\n",
    "    \"\"\"\n",
    "    Executes a pandas query string against the DataFrame in the context.\n",
    "    Handles both queries returning results and assignments/inplace modifications.\n",
    "\n",
    "    Args:\n",
    "        ctx (Context): The workflow context containing 'dataframe' and 'query_engine'.\n",
    "        query_str (str): The pandas query/command to execute. Must use 'df'.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        query_engine: PandasQueryEngine = await ctx.get(\"query_engine\")\n",
    "        df: pd.DataFrame = await ctx.get(\"dataframe\") # Get current DataFrame\n",
    "\n",
    "        if df is None:\n",
    "             return \"Error: DataFrame not found in context.\"\n",
    "        if query_engine is None:\n",
    "             # Modifications might still work without engine, but queries won't.\n",
    "             # Let's require engine for consistency for now.\n",
    "             return \"Error: PandasQueryEngine not found in context.\"\n",
    "\n",
    "        print(f\"Tool executing query: {query_str}\")\n",
    "\n",
    "        # Heuristic to detect modification (may need refinement)\n",
    "        is_modification = '=' in query_str or 'inplace=True' in query_str\n",
    "\n",
    "        if is_modification:\n",
    "            try:\n",
    "                # Prepare execution environment for modification\n",
    "                local_vars = {'df': df.copy()} # Use a copy to avoid modifying original during exec\n",
    "                global_vars = {'pd': pd} # Provide pandas module\n",
    "\n",
    "                # Execute the potentially multi-line modification code\n",
    "                exec(query_str, global_vars, local_vars)\n",
    "\n",
    "                # Get the modified DataFrame from the local scope of exec\n",
    "                modified_df = local_vars['df']\n",
    "\n",
    "                # Update the DataFrame in the workflow context\n",
    "                await ctx.set(\"dataframe\", modified_df)\n",
    "\n",
    "                # Attempt to update the query engine's internal DataFrame state\n",
    "                try:\n",
    "                    # This assumes the experimental engine uses _df internally\n",
    "                    query_engine._df = modified_df\n",
    "                except AttributeError:\n",
    "                     print(\"Warning: Could not directly update query_engine._df. Engine might use stale data for subsequent queries.\")\n",
    "\n",
    "                result = \"Executed modification successfully.\"\n",
    "                print(f\"Tool modification result: {result}\")\n",
    "                return result\n",
    "\n",
    "            except Exception as e:\n",
    "                # Log the full traceback for debugging exec errors\n",
    "                import traceback\n",
    "                print(f\"Tool exec error for query '{query_str}': {e}\\n{traceback.format_exc()}\")\n",
    "                error_msg = f\"Error executing modification '{query_str}': {e}\"\n",
    "                # Handle specific FutureWarning for fillna inplace\n",
    "                if \"FutureWarning\" in str(e) and \"fillna\" in query_str and \"inplace=True\" in query_str:\n",
    "                    alt_query_str = query_str.replace(\".fillna(\", \"['Time'].fillna(\").replace(\", inplace=True)\", \"\") # Basic attempt\n",
    "                    alt_query_str = f\"df['Time'] = df{alt_query_str}\" # Assign back\n",
    "                    print(f\"Attempting alternative syntax for fillna: {alt_query_str}\")\n",
    "                    try:\n",
    "                        local_vars = {'df': df.copy()}\n",
    "                        global_vars = {'pd': pd}\n",
    "                        exec(alt_query_str, global_vars, local_vars)\n",
    "                        modified_df = local_vars['df']\n",
    "                        await ctx.set(\"dataframe\", modified_df)\n",
    "                        query_engine._df = modified_df\n",
    "                        result = \"Executed modification successfully (alternative fillna syntax).\"\n",
    "                        print(f\"Tool modification result: {result}\")\n",
    "                        return result\n",
    "                    except Exception as e_alt:\n",
    "                         print(f\"Alternative fillna syntax failed: {e_alt}\")\n",
    "                         # Fall through to return original error message\n",
    "\n",
    "                return error_msg # Return original error if no specific handling worked\n",
    "        else:\n",
    "            # Assume it's a query returning a result\n",
    "            try:\n",
    "                # The query engine should use the updated _df assigned above\n",
    "                response = query_engine.query(query_str)\n",
    "                result = str(response)\n",
    "                # Check for known error patterns from the engine's response string\n",
    "                if \"error\" in result.lower() and (\"syntax\" in result.lower() or \"invalid\" in result.lower() or \"Traceback\" in result):\n",
    "                     error_msg = f\"Query engine failed for '{query_str}': {result}\"\n",
    "                     print(error_msg)\n",
    "                     return error_msg\n",
    "                print(f\"Tool query result: {result[:500]}...\")\n",
    "                return result\n",
    "            except Exception as e:\n",
    "                 import traceback\n",
    "                 print(f\"Error during query_engine.query('{query_str}'): {e}\\n{traceback.format_exc()}\")\n",
    "                 error_msg = f\"Error during query_engine.query('{query_str}'): {e}\"\n",
    "                 return error_msg\n",
    "\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(f\"Tool general error processing query '{query_str}': {e}\\n{traceback.format_exc()}\")\n",
    "        error_msg = f\"Error processing query '{query_str}': {e}\"\n",
    "        return error_msg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ba2d4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this code in a new cell after the 'execute_pandas_query_tool' definition cell\n",
    "\n",
    "async def save_dataframe_tool(ctx: Context, file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Saves the current DataFrame in the context to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The full path (including filename) where the CSV should be saved.\n",
    "                         Example: 'C:/path/to/modified_data.csv'\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Retrieve the DataFrame from the context\n",
    "        df: pd.DataFrame = await ctx.get(\"dataframe\")\n",
    "        if df is None:\n",
    "            return \"Error: DataFrame not found in context.\"\n",
    "\n",
    "        # --- Ensure the directory exists ---\n",
    "        output_dir = os.path.dirname(file_path)\n",
    "        if output_dir: # Check if path includes a directory\n",
    "             os.makedirs(output_dir, exist_ok=True)\n",
    "        # --- End Ensure directory exists ---\n",
    "\n",
    "\n",
    "        print(f\"Tool attempting to save DataFrame to: {file_path}\")\n",
    "        # Save the DataFrame\n",
    "        df.to_csv(file_path, index=False)\n",
    "        result = f\"DataFrame successfully saved to {file_path}\"\n",
    "        print(result)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error saving DataFrame to '{file_path}': {e}\"\n",
    "        print(error_msg)\n",
    "        return error_msg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a02ddf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the agents - \n",
    "def create_agents():\n",
    "    \"\"\"Create and return the agents needed for our workflow\"\"\"\n",
    "\n",
    "   \n",
    "    # Update docstring for the bound function if necessary, or rely on the original\n",
    "    pandas_query_tool = FunctionTool.from_defaults(\n",
    "        async_fn=execute_pandas_query_tool,  # <-- use async_fn\n",
    "        name=\"execute_pandas_query_tool\",\n",
    "        description=execute_pandas_query_tool.__doc__\n",
    "    )\n",
    "    save_df_tool = FunctionTool.from_defaults(\n",
    "        async_fn=save_dataframe_tool,        # <-- use async_fn\n",
    "        name=\"save_dataframe_tool\",\n",
    "        description=save_dataframe_tool.__doc__\n",
    "    )\n",
    "\n",
    "\n",
    "    data_prep_agent = FunctionCallingAgent.from_tools(\n",
    "        tools=[],\n",
    "        llm=llm,\n",
    "        verbose=False,\n",
    "        system_prompt=\"You are a data preparation agent. Your job is to describe the necessary steps to clean, transform, and prepare data for analysis based on provided statistics. \"\n",
    "                     \"You handle tasks like dealing with missing values, normalizing data, feature engineering, and ensuring data quality.\"\n",
    "    )\n",
    "    \n",
    "    data_analysis_agent = FunctionCallingAgent.from_tools(\n",
    "        tools=[pandas_query_tool, save_df_tool],\n",
    "        llm=llm,\n",
    "        verbose=True,\n",
    "        # Updated system prompt\n",
    "        system_prompt=(\n",
    "            \"You are a data analysis agent. Your job is to:\\n\"\n",
    "            \"1. Receive a data preparation description.\\n\"\n",
    "            \"2. Generate and execute pandas commands (using 'df') via the 'execute_pandas_query_tool' to perform the described cleaning/modifications (e.g., imputation, outlier handling, typo correction).\\n\"\n",
    "            \"3. Perform further analysis on the MODIFIED data using the 'execute_pandas_query_tool'.\\n\"\n",
    "            \"4. Generate a concise Markdown report summarizing:\\n\"\n",
    "            \"    - The cleaning/modification steps you executed.\\n\"\n",
    "            \"    - Key findings from your analysis of the modified data.\\n\"\n",
    "            \"5. Save the MODIFIED DataFrame to a new CSV file using the 'save_dataframe_tool'. Name the file by appending '_modified' to the original filename (e.g., if original was data.csv, save as data_modified.csv).\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return data_prep_agent, data_analysis_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55d13caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DataPrepEvent(Event):\n",
    "    original_path: str\n",
    "    column_names: list[str]\n",
    "    \n",
    "class DataAnalysisEvent(Event):\n",
    "    prepared_data_description: str\n",
    "    original_path: str\n",
    "\n",
    "# Define our multi-agent workflow\n",
    "class DataAnalysisFlow(Workflow):\n",
    "    \n",
    "    @step\n",
    "    async def setup(self, ctx: Context, ev: StartEvent) -> DataPrepEvent:\n",
    "        \"\"\"Initialize the agents and setup the workflow\"\"\"\n",
    "\n",
    "       \n",
    "        # --- Load data and create Pandas Query Engine ---\n",
    "        try:\n",
    "            df = pd.read_csv(ev.dataset_path)\n",
    "            # Handle potential issues like missing values before creating the engine if needed\n",
    "            # df = df.fillna('NA') # Example: fill NaNs, adjust as necessary\n",
    "            \n",
    "            # Create the query engine, passing the LLM is recommended\n",
    "            query_engine = PandasQueryEngine(df=df, llm=llm, verbose=True) \n",
    "            \n",
    "            # Store the DataFrame and query engine in the context\n",
    "            await ctx.set(\"dataframe\", df)\n",
    "            await ctx.set(\"query_engine\", query_engine)\n",
    "            await ctx.set(\"original_path\", ev.dataset_path)\n",
    "            \n",
    "            print(f\"Successfully loaded {ev.dataset_path} and created PandasQueryEngine.\")\n",
    "            \n",
    "            self.data_prep_agent, self.data_analysis_agent = create_agents()\n",
    "\n",
    "            # Return event with basic info for the next step\n",
    "            return DataPrepEvent(\n",
    "                original_path=ev.dataset_path, \n",
    "                column_names=df.columns.tolist()\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error during setup: Failed to load {ev.dataset_path} or create engine. Error: {e}\")\n",
    "            # Need to decide how to handle errors - perhaps raise or return a specific error event\n",
    "            # For now, returning an empty event or raising might be options. Let's raise.\n",
    "            raise ValueError(f\"Setup failed: {e}\")\n",
    "        \n",
    "    @step\n",
    "    async def data_preparation(self, ctx: Context, ev: DataPrepEvent) -> DataAnalysisEvent:\n",
    "        \"\"\"Use the data prep agent to suggest cleaning/preparation based on schema.\"\"\"\n",
    "        query_engine: PandasQueryEngine = await ctx.get(\"query_engine\")\n",
    "        df: pd.DataFrame = await ctx.get(\"dataframe\")\n",
    "        \n",
    "        # Get more comprehensive initial info using the engine\n",
    "        try:\n",
    "            # --- CHANGE: Use describe(include='all') ---\n",
    "            initial_info = str(query_engine.query(\"Show the shape of the dataframe (number of rows and columns) and the output of df.describe(include='all')\"))\n",
    "            print(f\"--- Initial Info for Prep Agent ---\\n{initial_info}\\n------------------------------------\") \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not query initial info from engine: {e}\")\n",
    "            initial_info = f\"Columns: {ev.column_names}\"\n",
    "\n",
    "        # Ask the data preparation agent for preparation steps/description\n",
    "        # --- CHANGE: Refined Prompt ---\n",
    "        prep_prompt = (\n",
    "            f\"The dataset (from {ev.original_path}) has the following shape and summary statistics:\\n{initial_info}\\n\\n\" \n",
    "            f\"Based *only* on these statistics, describe the necessary data preparation steps. \"\n",
    "            f\"Specifically mention potential issues like outliers (e.g., in 'Distance' max value), missing values (e.g., count mismatch in 'Time'), \"\n",
    "            f\"and data quality issues in categorical columns (e.g., unique count vs expected for 'Mode', potential typos like 'Bas', 'Cra', 'Walt'). \"\n",
    "            f\"Suggest specific actions like imputation for 'Time', outlier investigation for 'Distance', and checking unique values/correcting typos in 'Mode'. \"\n",
    "            f\"Focus on describing *what* needs to be done and *why* based *strictly* on the provided stats. If no issues are apparent from the stats, state that clearly. ALWAYS provide a description.\"\n",
    "        )\n",
    "        result = self.data_prep_agent.chat(prep_prompt)\n",
    "        \n",
    "        prepared_data_description = None # Initialize\n",
    "        if hasattr(result, 'response'):\n",
    "            prepared_data_description = result.response\n",
    "            # --- CHANGE: Add check for None/empty response ---\n",
    "            if not prepared_data_description: \n",
    "                prepared_data_description = \"Agent returned an empty description despite the prompt.\"\n",
    "                print(\"Warning: Agent response attribute was empty.\")\n",
    "            # --- END CHANGE ---\n",
    "        else:\n",
    "            # Fallback in case the response structure is different\n",
    "            prepared_data_description = \"Could not extract data preparation description from agent response.\"\n",
    "            print(f\"Warning: Agent response does not have expected 'response' attribute. Full result: {result}\")\n",
    "        \n",
    "        # Optional: Print the description generated by the agent\n",
    "        print(f\"--- Prep Agent Description Output ---\\n{prepared_data_description}\\n------------------------------------\")\n",
    "        \n",
    "        await ctx.set(\"prepared_data_description\", prepared_data_description)\n",
    "        \n",
    "        return DataAnalysisEvent(\n",
    "            prepared_data_description=prepared_data_description,\n",
    "            original_path=ev.original_path\n",
    "        )\n",
    "    \n",
    "    @step\n",
    "    async def data_analysis(self, ctx: Context, ev: DataAnalysisEvent) -> StopEvent:\n",
    "        \"\"\"Use the analysis agent to perform modifications, analyze, report, and save.\"\"\"\n",
    "   \n",
    "        # Construct the path for the modified file\n",
    "        original_path = await ctx.get(\"original_path\")\n",
    "        path_parts = os.path.splitext(original_path)\n",
    "        modified_file_path = f\"{path_parts[0]}_modified{path_parts[1]}\"\n",
    "\n",
    "        # Prompt the agent to perform analysis using its tool\n",
    "        analysis_request = (\n",
    "            f\"The dataset originally came from: {original_path}\\n\"\n",
    "            f\"Here is the data preparation description outlining necessary changes:\\n\"\n",
    "            f\"<preparation_description>\\n{ev.prepared_data_description}\\n</preparation_description>\\n\\n\"\n",
    "            f\"Now, please perform the following actions:\\n\"\n",
    "            f\"1. Execute the necessary pandas commands (using 'df') to apply the cleaning/modifications described above. Use the 'execute_pandas_query_tool'. Handle potential errors gracefully.\\n\"\n",
    "            f\"2. After modifying the data, perform a brief analysis focusing on the impact of the changes (e.g., check 'Time' description after imputation, 'Mode' unique values after correction, effect of handling 'Distance' outliers if any were addressed). Use the 'execute_pandas_query_tool'.\\n\"\n",
    "            f\"3. Generate a Markdown report summarizing the modifications performed and the key analysis findings.\\n\"\n",
    "            f\"4. Save the modified DataFrame to the following path using the 'save_dataframe_tool': '{modified_file_path}'\"\n",
    "        )\n",
    "\n",
    "        print(f\"--- Prompting Data Analysis Agent ---\\n{analysis_request}\\n------------------------------------\")\n",
    "\n",
    "        # The agent will now use the tool internally based on the prompt\n",
    "        agent_response = self.data_analysis_agent.chat(analysis_request)\n",
    "\n",
    "        # Extract the final report text\n",
    "        final_report = \"Agent did not provide a valid report.\"\n",
    "        if hasattr(agent_response, 'response') and agent_response.response:\n",
    "             final_report = agent_response.response\n",
    "             # The agent's response should ideally be the Markdown report\n",
    "        else:\n",
    "             print(f\"Warning: Agent response might not be the expected report. Full result: {agent_response}\")\n",
    "             final_report = str(agent_response) # Fallback\n",
    "\n",
    "        print(f\"--- Data Analysis Agent Final Response (Report) ---\\n{final_report}\\n------------------------------------------\")\n",
    "\n",
    "        # Store the report\n",
    "        await ctx.set(\"final_report\", final_report)\n",
    "\n",
    "        # Return a StopEvent with the final report\n",
    "        # The DataFrame saving is handled by the agent via the tool\n",
    "        return StopEvent(result={\n",
    "            \"final_report\": final_report\n",
    "        })\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03e77965",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "async def run_workflow(dataset_path):\n",
    "    \"\"\"Run the data analysis workflow on the given dataset\"\"\"\n",
    "\n",
    "    # Initialize the workflow\n",
    "    # Increased timeout as query engine + LLM calls can take longer\n",
    "    workflow = DataAnalysisFlow(timeout=240, verbose=True) # Increased timeout further\n",
    "\n",
    "    # Run the workflow - still pass dataset_path to StartEvent\n",
    "    try:\n",
    "        # Agents are created in setup\n",
    "        handler = workflow.run(\n",
    "            dataset_path=dataset_path,\n",
    "        )\n",
    "\n",
    "        # Get the final result\n",
    "        final_result_dict = await handler\n",
    "\n",
    "        print(\"\\n==== Final Report ====\")\n",
    "        # Adjust keys based on the StopEvent result dictionary from the modified data_analysis step\n",
    "        final_report = final_result_dict.get('final_report', 'N/A')\n",
    "\n",
    "        print(final_report) # Print the Markdown report\n",
    "\n",
    "        return final_result_dict # Return the dictionary containing the report\n",
    "    except Exception as e:\n",
    "         print(f\"Workflow failed: {e}\")\n",
    "         # --- Add traceback for debugging ---\n",
    "         import traceback\n",
    "         traceback.print_exc()\n",
    "         # --- End traceback ---\n",
    "         return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "663b69ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step setup\n",
      "Successfully loaded C:\\Users\\anteb\\Desktop\\Courses\\Projects\\data_analysis_ai\\data_analysis_agent\\Commute_Times_V1.csv and created PandasQueryEngine.\n",
      "Step setup produced event DataPrepEvent\n",
      "Running step data_preparation\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "> Pandas Instructions:\n",
      "```\n",
      "(df.shape, df.describe(include='all'))\n",
      "```\n",
      "> Pandas Output: ((281, 4),               Case Mode    Distance        Time\n",
      "count   281.000000  281  281.000000  278.000000\n",
      "unique         NaN    9         NaN         NaN\n",
      "top            NaN  Car         NaN         NaN\n",
      "freq           NaN   84         NaN         NaN\n",
      "mean    140.978648  NaN    3.658007   19.622302\n",
      "std      81.287714  NaN    8.206031   13.720435\n",
      "min       1.000000  NaN    0.200000    2.000000\n",
      "25%      71.000000  NaN    1.700000   10.000000\n",
      "50%     141.000000  NaN    3.000000   16.000000\n",
      "75%     211.000000  NaN    4.200000   24.750000\n",
      "max     281.000000  NaN   99.000000   57.000000)\n",
      "--- Initial Info for Prep Agent ---\n",
      "((281, 4),               Case Mode    Distance        Time\n",
      "count   281.000000  281  281.000000  278.000000\n",
      "unique         NaN    9         NaN         NaN\n",
      "top            NaN  Car         NaN         NaN\n",
      "freq           NaN   84         NaN         NaN\n",
      "mean    140.978648  NaN    3.658007   19.622302\n",
      "std      81.287714  NaN    8.206031   13.720435\n",
      "min       1.000000  NaN    0.200000    2.000000\n",
      "25%      71.000000  NaN    1.700000   10.000000\n",
      "50%     141.000000  NaN    3.000000   16.000000\n",
      "75%     211.000000  NaN    4.200000   24.750000\n",
      "max     281.000000  NaN   99.000000   57.000000)\n",
      "------------------------------------\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "--- Prep Agent Description Output ---\n",
      "Based on the provided dataset statistics, here are the necessary data preparation steps:\n",
      "\n",
      "### 1. **Handling Missing Values in 'Time'**\n",
      "   - **Issue**: The 'Time' column has 278 non-missing values out of 281 rows, indicating 3 missing values.\n",
      "   - **Action**: Impute the missing values in the 'Time' column. Since 'Time' is a numerical variable, consider using either the mean (19.62) or median (16.00) for imputation. The choice depends on the data distribution; the median is often more robust to outliers.\n",
      "   - **Reason**: Missing values can lead to errors in analysis or machine learning models, so they need to be addressed.\n",
      "\n",
      "---\n",
      "\n",
      "### 2. **Outlier Investigation in 'Distance'**\n",
      "   - **Issue**: The 'Distance' column has a maximum value of 99.00, which is significantly higher than the 75th percentile (4.20). This suggests a potential outlier.\n",
      "   - **Action**: Investigate the rows with a 'Distance' value near the maximum (e.g., 99.00). Verify whether these values are valid or if they are data entry errors. If confirmed as outliers, consider handling them by capping, transforming, or excluding them, depending on the context.\n",
      "   - **Reason**: Outliers can skew the analysis and may not represent typical commuting behavior.\n",
      "\n",
      "---\n",
      "\n",
      "### 3. **Checking Unique Values and Correcting Typos in 'Mode'**\n",
      "   - **Issue**: The 'Mode' column has 9 unique values, which seems reasonable for a commuting dataset but may include potential typos or inconsistencies (e.g., 'Bas', 'Cra', 'Walt').\n",
      "   - **Action**: Extract and review the unique values in the 'Mode' column. Check for typos or inconsistent entries. For example:\n",
      "     - Standardize entries like 'Bas' to 'Bus', 'Cra' to 'Car', and 'Walt' to 'Walk' if these are confirmed as errors.\n",
      "   - **Reason**: Typos or inconsistent formatting in categorical data can lead to incorrect grouping or analysis.\n",
      "\n",
      "---\n",
      "\n",
      "### 4. **Normalizing or Transforming 'Distance' and 'Time'**\n",
      "   - **Issue**: The 'Distance' column has a high standard deviation (8.21) relative to its mean (3.66), and the 'Time' column also has a relatively high standard deviation (13.\n",
      "------------------------------------\n",
      "Step data_preparation produced event DataAnalysisEvent\n",
      "Running step data_analysis\n",
      "--- Prompting Data Analysis Agent ---\n",
      "The dataset originally came from: C:\\Users\\anteb\\Desktop\\Courses\\Projects\\data_analysis_ai\\data_analysis_agent\\Commute_Times_V1.csv\n",
      "Here is the data preparation description outlining necessary changes:\n",
      "<preparation_description>\n",
      "Based on the provided dataset statistics, here are the necessary data preparation steps:\n",
      "\n",
      "### 1. **Handling Missing Values in 'Time'**\n",
      "   - **Issue**: The 'Time' column has 278 non-missing values out of 281 rows, indicating 3 missing values.\n",
      "   - **Action**: Impute the missing values in the 'Time' column. Since 'Time' is a numerical variable, consider using either the mean (19.62) or median (16.00) for imputation. The choice depends on the data distribution; the median is often more robust to outliers.\n",
      "   - **Reason**: Missing values can lead to errors in analysis or machine learning models, so they need to be addressed.\n",
      "\n",
      "---\n",
      "\n",
      "### 2. **Outlier Investigation in 'Distance'**\n",
      "   - **Issue**: The 'Distance' column has a maximum value of 99.00, which is significantly higher than the 75th percentile (4.20). This suggests a potential outlier.\n",
      "   - **Action**: Investigate the rows with a 'Distance' value near the maximum (e.g., 99.00). Verify whether these values are valid or if they are data entry errors. If confirmed as outliers, consider handling them by capping, transforming, or excluding them, depending on the context.\n",
      "   - **Reason**: Outliers can skew the analysis and may not represent typical commuting behavior.\n",
      "\n",
      "---\n",
      "\n",
      "### 3. **Checking Unique Values and Correcting Typos in 'Mode'**\n",
      "   - **Issue**: The 'Mode' column has 9 unique values, which seems reasonable for a commuting dataset but may include potential typos or inconsistencies (e.g., 'Bas', 'Cra', 'Walt').\n",
      "   - **Action**: Extract and review the unique values in the 'Mode' column. Check for typos or inconsistent entries. For example:\n",
      "     - Standardize entries like 'Bas' to 'Bus', 'Cra' to 'Car', and 'Walt' to 'Walk' if these are confirmed as errors.\n",
      "   - **Reason**: Typos or inconsistent formatting in categorical data can lead to incorrect grouping or analysis.\n",
      "\n",
      "---\n",
      "\n",
      "### 4. **Normalizing or Transforming 'Distance' and 'Time'**\n",
      "   - **Issue**: The 'Distance' column has a high standard deviation (8.21) relative to its mean (3.66), and the 'Time' column also has a relatively high standard deviation (13.\n",
      "</preparation_description>\n",
      "\n",
      "Now, please perform the following actions:\n",
      "1. Execute the necessary pandas commands (using 'df') to apply the cleaning/modifications described above. Use the 'execute_pandas_query_tool'. Handle potential errors gracefully.\n",
      "2. After modifying the data, perform a brief analysis focusing on the impact of the changes (e.g., check 'Time' description after imputation, 'Mode' unique values after correction, effect of handling 'Distance' outliers if any were addressed). Use the 'execute_pandas_query_tool'.\n",
      "3. Generate a Markdown report summarizing the modifications performed and the key analysis findings.\n",
      "4. Save the modified DataFrame to the following path using the 'save_dataframe_tool': 'C:\\Users\\anteb\\Desktop\\Courses\\Projects\\data_analysis_ai\\data_analysis_agent\\Commute_Times_V1_modified.csv'\n",
      "------------------------------------\n",
      "> Running step 8c509bf2-c34f-4400-87d5-0d7a04d8f8a0. Step input: The dataset originally came from: C:\\Users\\anteb\\Desktop\\Courses\\Projects\\data_analysis_ai\\data_analysis_agent\\Commute_Times_V1.csv\n",
      "Here is the data preparation description outlining necessary changes:\n",
      "<preparation_description>\n",
      "Based on the provided dataset statistics, here are the necessary data preparation steps:\n",
      "\n",
      "### 1. **Handling Missing Values in 'Time'**\n",
      "   - **Issue**: The 'Time' column has 278 non-missing values out of 281 rows, indicating 3 missing values.\n",
      "   - **Action**: Impute the missing values in the 'Time' column. Since 'Time' is a numerical variable, consider using either the mean (19.62) or median (16.00) for imputation. The choice depends on the data distribution; the median is often more robust to outliers.\n",
      "   - **Reason**: Missing values can lead to errors in analysis or machine learning models, so they need to be addressed.\n",
      "\n",
      "---\n",
      "\n",
      "### 2. **Outlier Investigation in 'Distance'**\n",
      "   - **Issue**: The 'Distance' column has a maximum value of 99.00, which is significantly higher than the 75th percentile (4.20). This suggests a potential outlier.\n",
      "   - **Action**: Investigate the rows with a 'Distance' value near the maximum (e.g., 99.00). Verify whether these values are valid or if they are data entry errors. If confirmed as outliers, consider handling them by capping, transforming, or excluding them, depending on the context.\n",
      "   - **Reason**: Outliers can skew the analysis and may not represent typical commuting behavior.\n",
      "\n",
      "---\n",
      "\n",
      "### 3. **Checking Unique Values and Correcting Typos in 'Mode'**\n",
      "   - **Issue**: The 'Mode' column has 9 unique values, which seems reasonable for a commuting dataset but may include potential typos or inconsistencies (e.g., 'Bas', 'Cra', 'Walt').\n",
      "   - **Action**: Extract and review the unique values in the 'Mode' column. Check for typos or inconsistent entries. For example:\n",
      "     - Standardize entries like 'Bas' to 'Bus', 'Cra' to 'Car', and 'Walt' to 'Walk' if these are confirmed as errors.\n",
      "   - **Reason**: Typos or inconsistent formatting in categorical data can lead to incorrect grouping or analysis.\n",
      "\n",
      "---\n",
      "\n",
      "### 4. **Normalizing or Transforming 'Distance' and 'Time'**\n",
      "   - **Issue**: The 'Distance' column has a high standard deviation (8.21) relative to its mean (3.66), and the 'Time' column also has a relatively high standard deviation (13.\n",
      "</preparation_description>\n",
      "\n",
      "Now, please perform the following actions:\n",
      "1. Execute the necessary pandas commands (using 'df') to apply the cleaning/modifications described above. Use the 'execute_pandas_query_tool'. Handle potential errors gracefully.\n",
      "2. After modifying the data, perform a brief analysis focusing on the impact of the changes (e.g., check 'Time' description after imputation, 'Mode' unique values after correction, effect of handling 'Distance' outliers if any were addressed). Use the 'execute_pandas_query_tool'.\n",
      "3. Generate a Markdown report summarizing the modifications performed and the key analysis findings.\n",
      "4. Save the modified DataFrame to the following path using the 'save_dataframe_tool': 'C:\\Users\\anteb\\Desktop\\Courses\\Projects\\data_analysis_ai\\data_analysis_agent\\Commute_Times_V1_modified.csv'\n",
      "Added user message to memory: The dataset originally came from: C:\\Users\\anteb\\Desktop\\Courses\\Projects\\data_analysis_ai\\data_analysis_agent\\Commute_Times_V1.csv\n",
      "Here is the data preparation description outlining necessary changes:\n",
      "<preparation_description>\n",
      "Based on the provided dataset statistics, here are the necessary data preparation steps:\n",
      "\n",
      "### 1. **Handling Missing Values in 'Time'**\n",
      "   - **Issue**: The 'Time' column has 278 non-missing values out of 281 rows, indicating 3 missing values.\n",
      "   - **Action**: Impute the missing values in the 'Time' column. Since 'Time' is a numerical variable, consider using either the mean (19.62) or median (16.00) for imputation. The choice depends on the data distribution; the median is often more robust to outliers.\n",
      "   - **Reason**: Missing values can lead to errors in analysis or machine learning models, so they need to be addressed.\n",
      "\n",
      "---\n",
      "\n",
      "### 2. **Outlier Investigation in 'Distance'**\n",
      "   - **Issue**: The 'Distance' column has a maximum value of 99.00, which is significantly higher than the 75th percentile (4.20). This suggests a potential outlier.\n",
      "   - **Action**: Investigate the rows with a 'Distance' value near the maximum (e.g., 99.00). Verify whether these values are valid or if they are data entry errors. If confirmed as outliers, consider handling them by capping, transforming, or excluding them, depending on the context.\n",
      "   - **Reason**: Outliers can skew the analysis and may not represent typical commuting behavior.\n",
      "\n",
      "---\n",
      "\n",
      "### 3. **Checking Unique Values and Correcting Typos in 'Mode'**\n",
      "   - **Issue**: The 'Mode' column has 9 unique values, which seems reasonable for a commuting dataset but may include potential typos or inconsistencies (e.g., 'Bas', 'Cra', 'Walt').\n",
      "   - **Action**: Extract and review the unique values in the 'Mode' column. Check for typos or inconsistent entries. For example:\n",
      "     - Standardize entries like 'Bas' to 'Bus', 'Cra' to 'Car', and 'Walt' to 'Walk' if these are confirmed as errors.\n",
      "   - **Reason**: Typos or inconsistent formatting in categorical data can lead to incorrect grouping or analysis.\n",
      "\n",
      "---\n",
      "\n",
      "### 4. **Normalizing or Transforming 'Distance' and 'Time'**\n",
      "   - **Issue**: The 'Distance' column has a high standard deviation (8.21) relative to its mean (3.66), and the 'Time' column also has a relatively high standard deviation (13.\n",
      "</preparation_description>\n",
      "\n",
      "Now, please perform the following actions:\n",
      "1. Execute the necessary pandas commands (using 'df') to apply the cleaning/modifications described above. Use the 'execute_pandas_query_tool'. Handle potential errors gracefully.\n",
      "2. After modifying the data, perform a brief analysis focusing on the impact of the changes (e.g., check 'Time' description after imputation, 'Mode' unique values after correction, effect of handling 'Distance' outliers if any were addressed). Use the 'execute_pandas_query_tool'.\n",
      "3. Generate a Markdown report summarizing the modifications performed and the key analysis findings.\n",
      "4. Save the modified DataFrame to the following path using the 'save_dataframe_tool': 'C:\\Users\\anteb\\Desktop\\Courses\\Projects\\data_analysis_ai\\data_analysis_agent\\Commute_Times_V1_modified.csv'\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "=== Calling Function ===\n",
      "Calling function: execute_pandas_query_tool with args: {\"query_str\": \"# Step 1: Impute missing values in 'Time' with the median value (16).\\nmedian_time = df['Time'].median()\\ndf['Time'].fillna(median_time, inplace=True)\\n\\n# Step 2: Investigate and handle outliers in 'Distance'.\\noutlier_threshold = 4.20\\ndistance_outliers = df[df['Distance'] > 4.20]\\n# Cap the outliers to the 75th percentile value\\ndf.loc[df['Distance'] > 4.20, 'Distance'] = 4.20\\n\\n# Step 3: Correct typos in 'Mode'.\\nmode_corrections = {'Bas': 'Bus', 'Cra': 'Car', 'Walt': 'Walk'}\\ndf['Mode'] = df['Mode'].replace(mode_corrections)\\n\\n# Step 4: Normalize 'Distance' and 'Time' columns.\\nfrom sklearn.preprocessing import MinMaxScaler\\nscaler = MinMaxScaler()\\ndf[['Distance', 'Time']] = scaler.fit_transform(df[['Distance', 'Time']])\"}\n",
      "=== Function Output ===\n",
      "Encountered error: Context is required for this tool\n",
      "> Running step f6949ce8-0c4f-4011-a8f3-a509cfc7c9c6. Step input: None\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "=== Calling Function ===\n",
      "Calling function: execute_pandas_query_tool with args: {\"query_str\": \"# Step 1: Impute missing values in 'Time' with the median value (16).\\nmedian_time = df['Time'].median()\\ndf['Time'].fillna(median_time, inplace=True)\"}\n",
      "=== Function Output ===\n",
      "Encountered error: Context is required for this tool\n",
      "> Running step b79bef6a-9b3f-47b3-a6ec-f55f6845630f. Step input: None\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "=== LLM Response ===\n",
      "It seems I need to execute the commands as part of a valid workflow context. Let me retry the steps properly.\n",
      "=== Calling Function ===\n",
      "Calling function: execute_pandas_query_tool with args: {\"query_str\": \"# Step 1: Impute missing values in 'Time' with the median value (16).\\nmedian_time = df['Time'].median()\\ndf['Time'].fillna(median_time, inplace=True)\"}\n",
      "=== Function Output ===\n",
      "Encountered error: Context is required for this tool\n",
      "=== Calling Function ===\n",
      "Calling function: execute_pandas_query_tool with args: {\"query_str\": \"# Step 2: Investigate and handle outliers in 'Distance'.\\noutlier_threshold = 4.20\\ndistance_outliers = df[df['Distance'] > 4.20]\\n# Cap the outliers to the 75th percentile value\\ndf.loc[df['Distance'] > 4.20, 'Distance'] = 4.20\"}\n",
      "=== Function Output ===\n",
      "Encountered error: Context is required for this tool\n",
      "=== Calling Function ===\n",
      "Calling function: execute_pandas_query_tool with args: {\"query_str\": \"# Step 3: Correct typos in 'Mode'.\\nmode_corrections = {'Bas': 'Bus', 'Cra': 'Car', 'Walt': 'Walk'}\\ndf['Mode'] = df['Mode'].replace(mode_corrections)\"}\n",
      "=== Function Output ===\n",
      "Encountered error: Context is required for this tool\n",
      "=== Calling Function ===\n",
      "Calling function: execute_pandas_query_tool with args: {\"query_str\": \"# Step 4: Normalize 'Distance' and 'Time' columns.\\nfrom sklearn.preprocessing import MinMaxScaler\\nscaler = MinMaxScaler()\\ndf[['Distance', 'Time']] = scaler.fit_transform(df[['Distance', 'Time']])\"}\n",
      "=== Function Output ===\n",
      "Encountered error: Context is required for this tool\n",
      "> Running step 597aa784-1b22-4de2-92e8-51e64d5a525f. Step input: None\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "=== LLM Response ===\n",
      "It seems I am unable to proceed with these operations because the required context is missing. Could you please ensure that the dataset is properly loaded into the context so I can perform the modifications?\n",
      "--- Data Analysis Agent Final Response (Report) ---\n",
      "It seems I am unable to proceed with these operations because the required context is missing. Could you please ensure that the dataset is properly loaded into the context so I can perform the modifications?\n",
      "------------------------------------------\n",
      "Step data_analysis produced event StopEvent\n",
      "\n",
      "==== Final Report ====\n",
      "It seems I am unable to proceed with these operations because the required context is missing. Could you please ensure that the dataset is properly loaded into the context so I can perform the modifications?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'final_report': 'It seems I am unable to proceed with these operations because the required context is missing. Could you please ensure that the dataset is properly loaded into the context so I can perform the modifications?'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example: Run the workflow with a CSV file\n",
    "# You'll need to update this with your actual dataset path\n",
    "dataset_path = r\"C:\\Users\\anteb\\Desktop\\Courses\\Projects\\data_analysis_ai\\data_analysis_agent\\Commute_Times_V1.csv\"\n",
    "\n",
    "# Execute the workflow\n",
    "await run_workflow(dataset_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
