{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97370ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install llama-index llama-index-embeddings-openai qdrant-client llama-index-vector-stores-qdrant llama-index llama-index-llms-openai llama-index-vector-stores-faiss faiss-cpu llama-index-llms-anthropic tavily-python llama-index-experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d72ed41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nest_asyncio\n",
    "from getpass import getpass\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Import LlamaIndex components\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.agent import FunctionCallingAgent\n",
    "from llama_index.core.workflow import Event, Workflow, Context, StopEvent, step\n",
    "from llama_index.core.workflow import StartEvent\n",
    "\n",
    "\n",
    "# Load environment variables and apply nest_asyncio for async operations\n",
    "load_dotenv()\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Get API keys\n",
    "OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY') or getpass(\"Enter OPENAI_API_KEY: \")\n",
    "\n",
    "# Initialize LLM\n",
    "llm = OpenAI(model=\"gpt-4o-2024-11-20\", api_key=OPENAI_API_KEY, temperature=0.5, max_tokens=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d13caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from llama_index.experimental.query_engine import PandasQueryEngine \n",
    "\n",
    "\n",
    "class DataPrepEvent(Event):\n",
    "    original_path: str\n",
    "    column_names: list[str]\n",
    "    \n",
    "class DataAnalysisEvent(Event):\n",
    "    prepared_data_description: str\n",
    "    original_path: str\n",
    "\n",
    "# Define our multi-agent workflow\n",
    "class DataAnalysisFlow(Workflow):\n",
    "    \n",
    "    @step\n",
    "    async def setup(self, ctx: Context, ev: StartEvent) -> DataPrepEvent:\n",
    "        \"\"\"Initialize the agents and setup the workflow\"\"\"\n",
    "        # Store the agents from the StartEvent\n",
    "        self.data_prep_agent = ev.data_prep_agent\n",
    "        self.data_analysis_agent = ev.data_analysis_agent\n",
    "        \n",
    "       \n",
    "        # --- Load data and create Pandas Query Engine ---\n",
    "        try:\n",
    "            df = pd.read_csv(ev.dataset_path)\n",
    "            # Handle potential issues like missing values before creating the engine if needed\n",
    "            # df = df.fillna('NA') # Example: fill NaNs, adjust as necessary\n",
    "            \n",
    "            # Create the query engine, passing the LLM is recommended\n",
    "            query_engine = PandasQueryEngine(df=df, llm=llm, verbose=True) \n",
    "            \n",
    "            # Store the DataFrame and query engine in the context\n",
    "            await ctx.set(\"dataframe\", df)\n",
    "            await ctx.set(\"query_engine\", query_engine)\n",
    "            await ctx.set(\"original_path\", ev.dataset_path)\n",
    "            \n",
    "            print(f\"Successfully loaded {ev.dataset_path} and created PandasQueryEngine.\")\n",
    "            \n",
    "            # Return event with basic info for the next step\n",
    "            return DataPrepEvent(\n",
    "                original_path=ev.dataset_path, \n",
    "                column_names=df.columns.tolist()\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error during setup: Failed to load {ev.dataset_path} or create engine. Error: {e}\")\n",
    "            # Need to decide how to handle errors - perhaps raise or return a specific error event\n",
    "            # For now, returning an empty event or raising might be options. Let's raise.\n",
    "            raise ValueError(f\"Setup failed: {e}\")\n",
    "        \n",
    "    @step\n",
    "    async def data_preparation(self, ctx: Context, ev: DataPrepEvent) -> DataAnalysisEvent:\n",
    "        \"\"\"Use the data prep agent to suggest cleaning/preparation based on schema.\"\"\"\n",
    "        query_engine: PandasQueryEngine = await ctx.get(\"query_engine\")\n",
    "        df: pd.DataFrame = await ctx.get(\"dataframe\")\n",
    "        \n",
    "        # Get more comprehensive initial info using the engine\n",
    "        try:\n",
    "            # --- CHANGE: Use describe(include='all') ---\n",
    "            initial_info = str(query_engine.query(\"Show the shape of the dataframe (number of rows and columns) and the output of df.describe(include='all')\"))\n",
    "            print(f\"--- Initial Info for Prep Agent ---\\n{initial_info}\\n------------------------------------\") \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not query initial info from engine: {e}\")\n",
    "            initial_info = f\"Columns: {ev.column_names}\"\n",
    "\n",
    "        # Ask the data preparation agent for preparation steps/description\n",
    "        # --- CHANGE: Refined Prompt ---\n",
    "        prep_prompt = (\n",
    "            f\"The dataset (from {ev.original_path}) has the following shape and summary statistics:\\n{initial_info}\\n\\n\" \n",
    "            f\"Based *only* on these statistics, describe the necessary data preparation steps. \"\n",
    "            f\"Specifically mention potential issues like outliers (e.g., in 'Distance' max value), missing values (e.g., count mismatch in 'Time'), \"\n",
    "            f\"and data quality issues in categorical columns (e.g., unique count vs expected for 'Mode', potential typos like 'Bas', 'Cra', 'Walt'). \"\n",
    "            f\"Suggest specific actions like imputation for 'Time', outlier investigation for 'Distance', and checking unique values/correcting typos in 'Mode'. \"\n",
    "            f\"Focus on describing *what* needs to be done and *why* based *strictly* on the provided stats. If no issues are apparent from the stats, state that clearly. ALWAYS provide a description.\"\n",
    "        )\n",
    "        result = self.data_prep_agent.chat(prep_prompt)\n",
    "        \n",
    "        prepared_data_description = None # Initialize\n",
    "        if hasattr(result, 'response'):\n",
    "            prepared_data_description = result.response\n",
    "            # --- CHANGE: Add check for None/empty response ---\n",
    "            if not prepared_data_description: \n",
    "                prepared_data_description = \"Agent returned an empty description despite the prompt.\"\n",
    "                print(\"Warning: Agent response attribute was empty.\")\n",
    "            # --- END CHANGE ---\n",
    "        else:\n",
    "            # Fallback in case the response structure is different\n",
    "            prepared_data_description = \"Could not extract data preparation description from agent response.\"\n",
    "            print(f\"Warning: Agent response does not have expected 'response' attribute. Full result: {result}\")\n",
    "        \n",
    "        # Optional: Print the description generated by the agent\n",
    "        print(f\"--- Prep Agent Description Output ---\\n{prepared_data_description}\\n------------------------------------\")\n",
    "        \n",
    "        await ctx.set(\"prepared_data_description\", prepared_data_description)\n",
    "        \n",
    "        return DataAnalysisEvent(\n",
    "            prepared_data_description=prepared_data_description,\n",
    "            original_path=ev.original_path\n",
    "        )\n",
    "    \n",
    "    @step\n",
    "    async def data_analysis(self, ctx: Context, ev: DataAnalysisEvent) -> StopEvent:\n",
    "        \"\"\"Use the analysis agent to generate and execute queries via the PandasQueryEngine.\"\"\"\n",
    "        query_engine: PandasQueryEngine = await ctx.get(\"query_engine\")\n",
    "        \n",
    "        # Updated analysis query request prompt\n",
    "        analysis_query_request = (\n",
    "            f\"Based on the dataset (from {ev.original_path}) and the following preparation description:\\n\"\n",
    "            f\"<preparation_description>\\n{ev.prepared_data_description}\\n</preparation_description>\\n\\n\"\n",
    "            f\"Generate a concise analysis plan (what key insights to look for). \"\n",
    "            # --- CHANGE: Added instruction for complete queries ---\n",
    "            f\"Then, provide 2-3 specific, COMPLETE, single-line pandas queries (using the 'df' variable) \"\n",
    "            f\"to execute via a query engine to find these insights. Ensure each query is valid pandas code. \"\n",
    "            f\"Enclose each query strictly within <query> and </query> tags. \"\n",
    "            f\"Example: <query>df.describe(include='all')</query>\\n<query>df.groupby('Mode')['Time'].mean()</query>\"\n",
    "        )\n",
    "        \n",
    "        agent_response = self.data_analysis_agent.chat(analysis_query_request)\n",
    "\n",
    "        # --- CHANGE: Properly extract response text ---\n",
    "        agent_response_text = \"Agent did not provide a valid analysis plan.\" # Default fallback\n",
    "        if hasattr(agent_response, 'response') and agent_response.response:\n",
    "             agent_response_text = agent_response.response\n",
    "        else:\n",
    "             print(f\"Warning: Agent response does not have expected 'response' attribute. Full result: {agent_response}\")\n",
    "        # --- END CHANGE ---\n",
    "\n",
    "        print(f\"--- Agent suggested analysis plan and queries ---\\n{agent_response_text}\\n-------------------------------------------------\") \n",
    "        \n",
    "        # Use regex to find queries within <query> tags\n",
    "        potential_queries = re.findall(r\"<query>(.*?)</query>\", agent_response_text) # Use checked text\n",
    "\n",
    "        # --- CHANGE: Extract plan separately for cleaner final output ---\n",
    "        # Attempt to extract text before the first query tag as the plan\n",
    "        plan_text = agent_response_text.split('<query>')[0].strip()\n",
    "        if not plan_text: # Fallback if split fails or plan is empty\n",
    "            plan_text = \"Analysis plan not explicitly extracted.\"\n",
    "        # --- END CHANGE ---\n",
    "\n",
    "        analysis_results_summary = f\"Agent Analysis Plan:\\n{plan_text}\\n\\nQuery Engine Results:\\n\" # Use extracted plan\n",
    "\n",
    "        if not potential_queries:\n",
    "             # Fallback if no tagged queries are found\n",
    "             print(\"Warning: No queries found within <query> tags. Falling back to df.describe().\")\n",
    "             potential_queries.append(\"df.describe()\") \n",
    "\n",
    "        print(f\"--- Attempting to execute queries: {potential_queries} ---\")\n",
    "        executed_queries_count = 0\n",
    "        # ... (rest of the query execution loop remains the same) ...\n",
    "\n",
    "        await ctx.set(\"analysis_results\", analysis_results_summary)\n",
    "        \n",
    "        # Return a StopEvent with the final result description\n",
    "        return StopEvent(result={\n",
    "            \"prepared_data_description\": ev.prepared_data_description,\n",
    "            \"analysis_results\": analysis_results_summary # Contains plan and query results\n",
    "        })\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d858630",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.core.workflow import Context\n",
    "\n",
    "\n",
    "async def execute_pandas_query(ctx: Context, query_str: str) -> str:\n",
    "    \"\"\"\n",
    "    Executes a single-line pandas query string against the DataFrame.\n",
    "    Use this tool to perform data analysis tasks like describing data,\n",
    "    grouping, filtering, calculating means, finding unique values, etc.\n",
    "\n",
    "    Args:\n",
    "        query_str (str): The pandas query to execute (e.g., \"df.describe()\", \"df['Mode'].unique()\").\n",
    "                         Must use 'df' as the DataFrame variable.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Retrieve the query engine from the context\n",
    "        query_engine: PandasQueryEngine = await ctx.get(\"query_engine\")\n",
    "        if not query_engine:\n",
    "            return \"Error: PandasQueryEngine not found in context.\"\n",
    "\n",
    "        print(f\"Tool executing query: {query_str}\")\n",
    "        response = query_engine.query(query_str)\n",
    "        result = str(response)\n",
    "        print(f\"Tool query result: {result[:500]}...\") # Print truncated result\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Tool query error: {e}\")\n",
    "        return f\"Error executing query '{query_str}': {e}\"\n",
    "\n",
    "\n",
    "pandas_query_tool = FunctionTool.from_defaults(fn=execute_pandas_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41ee1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the agents - \n",
    "def create_agents():\n",
    "    \"\"\"Create and return the agents needed for our workflow\"\"\"\n",
    "    data_prep_agent = FunctionCallingAgent.from_tools(\n",
    "        tools=[],\n",
    "        llm=llm,\n",
    "        verbose=False,\n",
    "        system_prompt=\"You are a data preparation agent. Your job is to describe the necessary steps to clean, transform, and prepare data for analysis based on provided statistics. \"\n",
    "                     \"You handle tasks like dealing with missing values, normalizing data, feature engineering, and ensuring data quality.\"\n",
    "    )\n",
    "    \n",
    "    data_analysis_agent = FunctionCallingAgent.from_tools(\n",
    "        tools=[pandas_query_tool],\n",
    "        llm=llm,\n",
    "        verbose=True,\n",
    "        # Updated system prompt\n",
    "        system_prompt=(\n",
    "            \"You are a data analysis agent. Your job is to analyze data based on a preparation description. \"\n",
    "            \"You identify patterns, correlations, and insights. \"\n",
    "            \"Use the 'execute_pandas_query' tool to run pandas queries against the DataFrame ('df') to find answers. \"\n",
    "            \"Plan your analysis steps and then use the tool to execute necessary queries. \"\n",
    "            \"Summarize your findings based on the tool's results.\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return data_prep_agent, data_analysis_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03e77965",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_workflow(dataset_path):\n",
    "    \"\"\"Run the data analysis workflow on the given dataset\"\"\"\n",
    "    # Create the agents\n",
    "    data_prep_agent, data_analysis_agent = create_agents()\n",
    "    \n",
    "    # Initialize the workflow\n",
    "    # Increased timeout as query engine + LLM calls can take longer\n",
    "    workflow = DataAnalysisFlow(timeout=180, verbose=True) \n",
    "    \n",
    "    # Run the workflow - still pass dataset_path to StartEvent\n",
    "    try:\n",
    "        handler = workflow.run(\n",
    "            dataset_path=dataset_path,\n",
    "            data_prep_agent=data_prep_agent,\n",
    "            data_analysis_agent=data_analysis_agent\n",
    "            # No need to pass content here, setup handles loading\n",
    "        )\n",
    "        \n",
    "        # Get the final result\n",
    "        final_result = await handler\n",
    "        \n",
    "        print(\"\\n==== Analysis Results ====\")\n",
    "        # Adjust keys based on the StopEvent result dictionary\n",
    "        prep_desc = final_result.get('prepared_data_description', 'N/A')\n",
    "        analysis_res = final_result.get('analysis_results', 'N/A')\n",
    "        \n",
    "        print(f\"1. Data Preparation Description:\\n{prep_desc}...\\n\") # Show more potentially\n",
    "        print(f\"2. Analysis Results (Plan & Queries):\\n{analysis_res}...\\n\") # Show more potentially\n",
    "        \n",
    "        return final_result\n",
    "    except Exception as e:\n",
    "         print(f\"Workflow failed: {e}\")\n",
    "         return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "663b69ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step setup\n",
      "Successfully loaded C:\\Users\\anteb\\Desktop\\Courses\\Projects\\data_analysis_ai\\data_analysis_agent\\Commute_Times_V1.csv and created PandasQueryEngine.\n",
      "Step setup produced event DataPrepEvent\n",
      "Running step data_preparation\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "> Pandas Instructions:\n",
      "```\n",
      "(df.shape, df.describe(include='all'))\n",
      "```\n",
      "> Pandas Output: ((281, 4),               Case Mode    Distance        Time\n",
      "count   281.000000  281  281.000000  278.000000\n",
      "unique         NaN    9         NaN         NaN\n",
      "top            NaN  Car         NaN         NaN\n",
      "freq           NaN   84         NaN         NaN\n",
      "mean    140.978648  NaN    3.658007   19.622302\n",
      "std      81.287714  NaN    8.206031   13.720435\n",
      "min       1.000000  NaN    0.200000    2.000000\n",
      "25%      71.000000  NaN    1.700000   10.000000\n",
      "50%     141.000000  NaN    3.000000   16.000000\n",
      "75%     211.000000  NaN    4.200000   24.750000\n",
      "max     281.000000  NaN   99.000000   57.000000)\n",
      "--- Initial Info for Prep Agent ---\n",
      "((281, 4),               Case Mode    Distance        Time\n",
      "count   281.000000  281  281.000000  278.000000\n",
      "unique         NaN    9         NaN         NaN\n",
      "top            NaN  Car         NaN         NaN\n",
      "freq           NaN   84         NaN         NaN\n",
      "mean    140.978648  NaN    3.658007   19.622302\n",
      "std      81.287714  NaN    8.206031   13.720435\n",
      "min       1.000000  NaN    0.200000    2.000000\n",
      "25%      71.000000  NaN    1.700000   10.000000\n",
      "50%     141.000000  NaN    3.000000   16.000000\n",
      "75%     211.000000  NaN    4.200000   24.750000\n",
      "max     281.000000  NaN   99.000000   57.000000)\n",
      "------------------------------------\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "--- Prep Agent Description Output ---\n",
      "Based on the provided summary statistics of the dataset, here are the necessary data preparation steps:\n",
      "\n",
      "### 1. **Handling Missing Values in 'Time' Column**\n",
      "   - **Issue**: The 'Time' column has 278 non-missing values, while the dataset has 281 rows. This indicates 3 missing values.\n",
      "   - **Action**: Impute the missing values in the 'Time' column. Since the 'Time' column appears to be continuous and skewed (mean is 19.62 and median is 16), use the median (16.0) for imputation to avoid being influenced by potential outliers.\n",
      "   - **Reason**: Imputation ensures no rows are excluded during analysis due to missing values.\n",
      "\n",
      "---\n",
      "\n",
      "### 2. **Investigating Outliers in 'Distance'**\n",
      "   - **Issue**: The maximum value for 'Distance' is 99, which is significantly higher than the 75th percentile (4.2). This suggests the presence of a potential outlier or data entry error.\n",
      "   - **Action**: Investigate the rows with 'Distance' values near the maximum (e.g., >75th percentile) to verify whether these values are valid. If they are confirmed to be errors, consider capping the values at a reasonable threshold or removing them.\n",
      "   - **Reason**: Outliers can skew analysis and may not accurately represent the population. Validating and addressing them ensures data quality.\n",
      "\n",
      "---\n",
      "\n",
      "### 3. **Examining and Correcting 'Mode' Column**\n",
      "   - **Issue**: The 'Mode' column has 9 unique values. Given that 'Mode' typically refers to transportation modes (e.g., Car, Bus, Walk), this number might be higher than expected due to potential typos or variations (e.g., 'Bas' instead of 'Bus', 'Cra' instead of 'Car', 'Walt' instead of 'Walk').\n",
      "   - **Action**: Check the unique values in the 'Mode' column for inconsistencies, typos, or unexpected categories. Standardize the values to ensure consistency (e.g., replace 'Bas' with 'Bus', 'Cra' with 'Car', etc.).\n",
      "   - **Reason**: Inconsistent or incorrect categories can lead to inaccurate analysis and grouping.\n",
      "\n",
      "---\n",
      "\n",
      "### 4. **Validating 'Case' Column**\n",
      "   - **Issue**: The 'Case' column appears to be a unique identifier based on its range (min=1, max=281) and count (281\n",
      "------------------------------------\n",
      "Step data_preparation produced event DataAnalysisEvent\n",
      "Running step data_analysis\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "--- Agent suggested analysis plan and queries ---\n",
      "### Analysis Plan:\n",
      "1. **Commute Time Analysis**:\n",
      "   - Investigate the central tendency (mean, median) and distribution of commute times after imputing missing values.\n",
      "   - Analyze the average commute time grouped by transportation mode to identify which mode is most time-efficient.\n",
      "\n",
      "2. **Outlier Analysis in Distance**:\n",
      "   - Identify and examine rows with 'Distance' values greater than the 75th percentile to determine if they are valid or errors.\n",
      "\n",
      "3. **Transportation Mode Consistency**:\n",
      "   - Check for inconsistencies or typos in the 'Mode' column and standardize them.\n",
      "\n",
      "---\n",
      "\n",
      "### Queries:\n",
      "\n",
      "1. Impute missing values in the 'Time' column with the median and describe the column to confirm:\n",
      "<query>df['Time'] = df['Time'].fillna(df['Time'].median()); df['Time'].describe()</query>\n",
      "\n",
      "2. Identify rows where 'Distance' exceeds the 75th percentile:\n",
      "<query>df[df['Distance'] > df['Distance'].quantile(0.75)]</query>\n",
      "\n",
      "3. List unique values in the 'Mode' column to check for inconsistencies:\n",
      "<query>df['Mode'].unique()</query>\n",
      "-------------------------------------------------\n",
      "--- Attempting to execute queries: [\"df['Time'] = df['Time'].fillna(df['Time'].median()); df['Time'].describe()\", \"df[df['Distance'] > df['Distance'].quantile(0.75)]\", \"df['Mode'].unique()\"] ---\n",
      "Step data_analysis produced event StopEvent\n",
      "\n",
      "==== Analysis Results ====\n",
      "1. Data Preparation Description:\n",
      "Based on the provided summary statistics of the dataset, here are the necessary data preparation steps:\n",
      "\n",
      "### 1. **Handling Missing Values in 'Time' Column**\n",
      "   - **Issue**: The 'Time' column has 278 non-missing values, while the dataset has 281 rows. This indicates 3 missing values.\n",
      "   - **Action**: Impute the missing values in the 'Time' column. Since the 'Time' column appears to be continuous and skewed (mean is 19.62 and median is 16), use the median (16.0) for imputation to avoid being influenced by potential outliers.\n",
      "   - **Reason**: Imputation ensures no rows are excluded during analysis due to missing values.\n",
      "\n",
      "---\n",
      "\n",
      "### 2. **Investigating Outliers in 'Distance'**\n",
      "   - **Issue**: The maximum value for 'Distance' is 99, which is significantly higher than the 75th percentile (4.2). This suggests the presence of a potential outlier or data entry error.\n",
      "   - **Action**: Investigate the rows with 'Distance' values near the maximum (e.g., >75th percentile) to verify whether these values are valid. If they are confirmed to be errors, consider capping the values at a reasonable threshold or removing them.\n",
      "   - **Reason**: Outliers can skew analysis and may not accurately represent the population. Validating and addressing them ensures data quality.\n",
      "\n",
      "---\n",
      "\n",
      "### 3. **Examining and Correcting 'Mode' Column**\n",
      "   - **Issue**: The 'Mode' column has 9 unique values. Given that 'Mode' typically refers to transportation modes (e.g., Car, Bus, Walk), this number might be higher than expected due to potential typos or variations (e.g., 'Bas' instead of 'Bus', 'Cra' instead of 'Car', 'Walt' instead of 'Walk').\n",
      "   - **Action**: Check the unique values in the 'Mode' column for inconsistencies, typos, or unexpected categories. Standardize the values to ensure consistency (e.g., replace 'Bas' with 'Bus', 'Cra' with 'Car', etc.).\n",
      "   - **Reason**: Inconsistent or incorrect categories can lead to inaccurate analysis and grouping.\n",
      "\n",
      "---\n",
      "\n",
      "### 4. **Validating 'Case' Column**\n",
      "   - **Issue**: The 'Case' column appears to be a unique identifier based on its range (min=1, max=281) and count (281...\n",
      "\n",
      "2. Analysis Results (Plan & Queries):\n",
      "Agent Analysis Plan:\n",
      "### Analysis Plan:\n",
      "1. **Commute Time Analysis**:\n",
      "   - Investigate the central tendency (mean, median) and distribution of commute times after imputing missing values.\n",
      "   - Analyze the average commute time grouped by transportation mode to identify which mode is most time-efficient.\n",
      "\n",
      "2. **Outlier Analysis in Distance**:\n",
      "   - Identify and examine rows with 'Distance' values greater than the 75th percentile to determine if they are valid or errors.\n",
      "\n",
      "3. **Transportation Mode Consistency**:\n",
      "   - Check for inconsistencies or typos in the 'Mode' column and standardize them.\n",
      "\n",
      "---\n",
      "\n",
      "### Queries:\n",
      "\n",
      "1. Impute missing values in the 'Time' column with the median and describe the column to confirm:\n",
      "\n",
      "Query Engine Results:\n",
      "...\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'prepared_data_description': \"Based on the provided summary statistics of the dataset, here are the necessary data preparation steps:\\n\\n### 1. **Handling Missing Values in 'Time' Column**\\n   - **Issue**: The 'Time' column has 278 non-missing values, while the dataset has 281 rows. This indicates 3 missing values.\\n   - **Action**: Impute the missing values in the 'Time' column. Since the 'Time' column appears to be continuous and skewed (mean is 19.62 and median is 16), use the median (16.0) for imputation to avoid being influenced by potential outliers.\\n   - **Reason**: Imputation ensures no rows are excluded during analysis due to missing values.\\n\\n---\\n\\n### 2. **Investigating Outliers in 'Distance'**\\n   - **Issue**: The maximum value for 'Distance' is 99, which is significantly higher than the 75th percentile (4.2). This suggests the presence of a potential outlier or data entry error.\\n   - **Action**: Investigate the rows with 'Distance' values near the maximum (e.g., >75th percentile) to verify whether these values are valid. If they are confirmed to be errors, consider capping the values at a reasonable threshold or removing them.\\n   - **Reason**: Outliers can skew analysis and may not accurately represent the population. Validating and addressing them ensures data quality.\\n\\n---\\n\\n### 3. **Examining and Correcting 'Mode' Column**\\n   - **Issue**: The 'Mode' column has 9 unique values. Given that 'Mode' typically refers to transportation modes (e.g., Car, Bus, Walk), this number might be higher than expected due to potential typos or variations (e.g., 'Bas' instead of 'Bus', 'Cra' instead of 'Car', 'Walt' instead of 'Walk').\\n   - **Action**: Check the unique values in the 'Mode' column for inconsistencies, typos, or unexpected categories. Standardize the values to ensure consistency (e.g., replace 'Bas' with 'Bus', 'Cra' with 'Car', etc.).\\n   - **Reason**: Inconsistent or incorrect categories can lead to inaccurate analysis and grouping.\\n\\n---\\n\\n### 4. **Validating 'Case' Column**\\n   - **Issue**: The 'Case' column appears to be a unique identifier based on its range (min=1, max=281) and count (281\",\n",
       " 'analysis_results': \"Agent Analysis Plan:\\n### Analysis Plan:\\n1. **Commute Time Analysis**:\\n   - Investigate the central tendency (mean, median) and distribution of commute times after imputing missing values.\\n   - Analyze the average commute time grouped by transportation mode to identify which mode is most time-efficient.\\n\\n2. **Outlier Analysis in Distance**:\\n   - Identify and examine rows with 'Distance' values greater than the 75th percentile to determine if they are valid or errors.\\n\\n3. **Transportation Mode Consistency**:\\n   - Check for inconsistencies or typos in the 'Mode' column and standardize them.\\n\\n---\\n\\n### Queries:\\n\\n1. Impute missing values in the 'Time' column with the median and describe the column to confirm:\\n\\nQuery Engine Results:\\n\"}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example: Run the workflow with a CSV file\n",
    "# You'll need to update this with your actual dataset path\n",
    "dataset_path = r\"C:\\Users\\anteb\\Desktop\\Courses\\Projects\\data_analysis_ai\\data_analysis_agent\\Commute_Times_V1.csv\"\n",
    "\n",
    "# Execute the workflow\n",
    "await run_workflow(dataset_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
