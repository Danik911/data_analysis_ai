{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97370ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install llama-index llama-index-embeddings-openai qdrant-client llama-index-vector-stores-qdrant llama-index llama-index-llms-openai llama-index-vector-stores-faiss faiss-cpu llama-index-llms-anthropic tavily-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d72ed41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nest_asyncio\n",
    "from getpass import getpass\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Import LlamaIndex components\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.agent import FunctionCallingAgent\n",
    "from llama_index.core.workflow import Event, Workflow, Context, StopEvent, step\n",
    "from llama_index.core.workflow import StartEvent\n",
    "\n",
    "\n",
    "# Load environment variables and apply nest_asyncio for async operations\n",
    "load_dotenv()\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Get API keys\n",
    "OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY') or getpass(\"Enter OPENAI_API_KEY: \")\n",
    "\n",
    "# Initialize LLM\n",
    "llm = OpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=0.5, max_tokens=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55d13caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define events for our workflow\n",
    "class DataPrepEvent(Event):\n",
    "    dataset_path: str\n",
    "    \n",
    "class DataAnalysisEvent(Event):\n",
    "    prepared_data: str\n",
    "    \n",
    "class DataVisualizationEvent(Event):\n",
    "    analysis_results: str\n",
    "    \n",
    "class VisualizationResults(Event):\n",
    "    visualizations: str\n",
    "\n",
    "# Define our multi-agent workflow\n",
    "class DataAnalysisFlow(Workflow):\n",
    "    \n",
    "    @step\n",
    "    async def setup(self, ev: StartEvent) -> DataPrepEvent:\n",
    "        \"\"\"Initialize the agents and setup the workflow\"\"\"\n",
    "        # Store the agents from the StartEvent\n",
    "        self.data_prep_agent = ev.data_prep_agent\n",
    "        self.data_analysis_agent = ev.data_analysis_agent\n",
    "        self.data_viz_agent = ev.data_viz_agent\n",
    "        \n",
    "        # Return the path to start data preparation\n",
    "        return DataPrepEvent(dataset_path=ev.dataset_path)\n",
    "    \n",
    "    @step\n",
    "    async def data_preparation(self, ctx: Context, ev: DataPrepEvent) -> DataAnalysisEvent:\n",
    "        \"\"\"Prepare the data for analysis\"\"\"\n",
    "        # Store the dataset path in context\n",
    "        await ctx.set(\"dataset_path\", ev.dataset_path)\n",
    "        \n",
    "        # Ask the data preparation agent to clean and prepare the data\n",
    "        result = self.data_prep_agent.chat(\n",
    "            f\"Prepare this dataset for analysis: {ev.dataset_path}. \"\n",
    "            f\"Perform cleaning, handling missing values, and feature engineering as needed.\"\n",
    "        )\n",
    "        \n",
    "        # Store the prepared data in context\n",
    "        prepared_data = str(result)\n",
    "        await ctx.set(\"prepared_data\", prepared_data)\n",
    "        \n",
    "        return DataAnalysisEvent(prepared_data=prepared_data)\n",
    "    \n",
    "    @step\n",
    "    async def data_analysis(self, ctx: Context, ev: DataAnalysisEvent) -> DataVisualizationEvent:\n",
    "        \"\"\"Analyze the prepared data\"\"\"\n",
    "        # Get the dataset path from context\n",
    "        dataset_path = await ctx.get(\"dataset_path\")\n",
    "        \n",
    "        # Ask the data analysis agent to analyze the data\n",
    "        result = self.data_analysis_agent.chat(\n",
    "            f\"Analyze this prepared data: <data>{ev.prepared_data}</data>. \"\n",
    "            f\"The original dataset is from: {dataset_path}. \"\n",
    "            f\"Perform statistical analysis, identify patterns, and extract insights.\"\n",
    "        )\n",
    "        \n",
    "        # Store the analysis results\n",
    "        analysis_results = str(result)\n",
    "        await ctx.set(\"analysis_results\", analysis_results)\n",
    "        \n",
    "        return DataVisualizationEvent(analysis_results=analysis_results)\n",
    "    \n",
    "    @step\n",
    "    async def data_visualization(self, ctx: Context, ev: DataVisualizationEvent) -> StopEvent:\n",
    "        \"\"\"Create visualizations based on the analysis\"\"\"\n",
    "        # Get dataset info from context\n",
    "        dataset_path = await ctx.get(\"dataset_path\")\n",
    "        prepared_data = await ctx.get(\"prepared_data\")\n",
    "        \n",
    "        # Ask the visualization agent to create visualizations\n",
    "        result = self.data_viz_agent.chat(\n",
    "            f\"Create visualizations for this analysis: <analysis>{ev.analysis_results}</analysis>. \"\n",
    "            f\"The data was prepared as follows: <prepared_data>{prepared_data}</prepared_data>. \"\n",
    "            f\"The original dataset is from: {dataset_path}. \"\n",
    "            f\"Suggest appropriate chart types and visualization techniques.\"\n",
    "        )\n",
    "        \n",
    "        # Send the visualization results as an event\n",
    "        visualizations = str(result)\n",
    "        ctx.write_event_to_stream(VisualizationResults(visualizations=visualizations))\n",
    "        \n",
    "        # Return a StopEvent with the final result\n",
    "        return StopEvent(result={\n",
    "            \"prepared_data\": prepared_data,\n",
    "            \"analysis_results\": ev.analysis_results,\n",
    "            \"visualizations\": visualizations\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e41ee1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the agents\n",
    "def create_agents():\n",
    "    \"\"\"Create and return the three agents needed for our workflow\"\"\"\n",
    "    data_prep_agent = FunctionCallingAgent.from_tools(\n",
    "        tools=[],\n",
    "        llm=llm,\n",
    "        verbose=False,\n",
    "        system_prompt=\"You are a data preparation agent. Your job is to clean, transform, and prepare data for analysis. \"\n",
    "                     \"You handle tasks like dealing with missing values, normalizing data, feature engineering, and ensuring data quality.\"\n",
    "    )\n",
    "    \n",
    "    data_analysis_agent = FunctionCallingAgent.from_tools(\n",
    "        tools=[],\n",
    "        llm=llm,\n",
    "        verbose=False,\n",
    "        system_prompt=\"You are a data analysis agent. Your job is to perform statistical analysis on prepared data. \"\n",
    "                     \"You identify patterns, correlations, and insights from the data to help answer business or research questions.\"\n",
    "    )\n",
    "    \n",
    "    data_viz_agent = FunctionCallingAgent.from_tools(\n",
    "        tools=[],\n",
    "        llm=llm,\n",
    "        verbose=False,\n",
    "        system_prompt=\"You are a data visualization agent. Your job is to recommend appropriate visualizations for data analysis results. \"\n",
    "                     \"You suggest chart types, plotting techniques, and visualization approaches to effectively communicate insights.\"\n",
    "    )\n",
    "    \n",
    "    return data_prep_agent, data_analysis_agent, data_viz_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e77965",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_workflow(dataset_path):\n",
    "    \"\"\"Run the data analysis workflow on the given dataset\"\"\"\n",
    "    # Create the agents\n",
    "    data_prep_agent, data_analysis_agent, data_viz_agent = create_agents()\n",
    "    \n",
    "    # Initialize the workflow\n",
    "    workflow = DataAnalysisFlow(timeout=60, verbose=True)\n",
    "    \n",
    "    # Run the workflow\n",
    "    handler = workflow.run(\n",
    "        dataset_path=dataset_path,\n",
    "        data_prep_agent=data_prep_agent,\n",
    "        data_analysis_agent=data_analysis_agent,\n",
    "        data_viz_agent=data_viz_agent\n",
    "    )\n",
    "    \n",
    "    # Process events from the workflow\n",
    "    async for ev in handler.stream_events():\n",
    "        if isinstance(ev, VisualizationResults):\n",
    "            print(\"==== Visualization Recommendations ====\")\n",
    "            print(ev.visualizations)\n",
    "    \n",
    "    # Get the final result\n",
    "    final_result = await handler\n",
    "    \n",
    "    print(\"\\n==== Complete Analysis Results ====\")\n",
    "    print(f\"1. Data Preparation:\\n{final_result['prepared_data'][:500]}...\\n\")\n",
    "    print(f\"2. Analysis Results:\\n{final_result['analysis_results'][:500]}...\\n\")\n",
    "    print(f\"3. Visualization Recommendations:\\n{final_result['visualizations'][:500]}...\\n\")\n",
    "    \n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663b69ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
