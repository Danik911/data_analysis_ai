{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f4a2c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install llama-index llama-index-embeddings-openai qdrant-client llama-index-vector-stores-qdrant llama-index llama-index-llms-openai llama-index-vector-stores-faiss faiss-cpu llama-index-llms-anthropic tavily-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e8396ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nest_asyncio\n",
    "from getpass import getpass\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(r\"C:\\Users\\anteb\\PycharmProjects\\JupyterProject\\.env\")\n",
    "nest_asyncio.apply()\n",
    "\n",
    "CO_API_KEY = os.environ.get('CO_API_KEY') or getpass(\"Enter CO_API_KEY: \")\n",
    "OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY') or getpass(\"Enter OPENAI_API_KEY: \")\n",
    "QDRANT_URL = os.environ.get('QDRANT_URL') or getpass(\"Enter QDRANT_URL: \")\n",
    "QDRANT_API_KEY = os.environ.get('QDRANT_API_KEY') or getpass(\"Enter QDRANT_API_KEY: \")\n",
    "TAVILY_API_KEY = os.environ.get('TAVILY_API_KEY') or getpass(\"Enter TAVILY_API_KEY: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce477171",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core import Settings\n",
    "\n",
    "llm = OpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=0.5, max_tokens=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a200c9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tavily import AsyncTavilyClient\n",
    "\n",
    "# note the type annotations for the incoming query and the return string\n",
    "async def search_web(query: str) -> str:\n",
    "    \"\"\"Useful for using the web to answer questions.\"\"\"\n",
    "    client = AsyncTavilyClient(api_key=TAVILY_API_KEY)\n",
    "    return str(await client.search(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3aa6abc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent.workflow import AgentWorkflow\n",
    "\n",
    "workflow = AgentWorkflow.from_tools_or_functions(\n",
    "    [search_web],\n",
    "    system_prompt=\"You are a helpful assistant that answers questions. If you don't know the answer, you can search the web for information.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a1a014b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current weather in Smolensk, Russia, is as follows:\n",
      "\n",
      "- **Temperature**: 7.7°C (45.9°F)\n",
      "- **Condition**: Sunny\n",
      "- **Wind**: 4.3 mph (6.8 kph) from the northwest\n",
      "- **Humidity**: 70%\n",
      "- **Visibility**: 10 km\n",
      "- **Pressure**: 1017 mb\n",
      "\n",
      "For more details, you can check the weather on [WeatherAPI](https://www.weatherapi.com/) or [AccuWeather](https://www.accuweather.com/en/ru/smolensk/295475/weather-forecast/295475).\n"
     ]
    }
   ],
   "source": [
    "response = await workflow.run(user_msg=\"What is the weather in Smolensk Russia?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db2a1fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice to meet you, Daniil! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.workflow import Context\n",
    "\n",
    "# configure a context to work with our workflow\n",
    "ctx = Context(workflow)\n",
    "\n",
    "response = await workflow.run(\n",
    "    user_msg=\"My name is Daniil, nice to meet you!\", ctx=ctx # give the configured context to the workflow\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1059c7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import JsonPickleSerializer, JsonSerializer\n",
    "\n",
    "# convert our Context to a dictionary object\n",
    "ctx_dict = ctx.to_dict(serializer=JsonSerializer())\n",
    "\n",
    "# create a new Context from the dictionary\n",
    "restored_ctx = Context.from_dict(\n",
    "    workflow, ctx_dict, serializer=JsonSerializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e84272cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, I remember your name, Daniil! How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "response = await workflow.run(\n",
    "    user_msg=\"Do you still remember my name?\", ctx=restored_ctx\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9051ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent.workflow import (\n",
    "    AgentInput,\n",
    "    AgentOutput,\n",
    "    ToolCall,\n",
    "    ToolCallResult,\n",
    "    AgentStream,\n",
    ")\n",
    "\n",
    "handler = workflow.run(user_msg=\"What is the weather in Smolenks? Call me by name when you anser\", ctx=restored_ctx)\n",
    "\n",
    "\n",
    "async for event in handler.stream_events():\n",
    "    if isinstance(event, AgentInput):\n",
    "       print(\"Agent input: \", event.input)  # the current input messages\n",
    "       print(\"Agent name:\", event.current_agent_name)  # the current agent name\n",
    "    elif isinstance(event, AgentOutput):\n",
    "       print(\"Agent output: \", event.response)  # the current full response\n",
    "       print(\"Tool calls made: \", event.tool_calls)  # the selected tool calls, if any\n",
    "       print(\"Raw LLM response: \", event.raw)  # the raw llm api response\n",
    "    elif isinstance(event, ToolCallResult):\n",
    "       print(\"Tool called: \", event.tool_name)  # the tool name\n",
    "       print(\"Arguments to the tool: \", event.tool_kwargs)  # the tool kwargs\n",
    "       print(\"Tool output: \", event.tool_output)  # the tool output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e42961d",
   "metadata": {},
   "source": [
    "## Tools and State\n",
    "\n",
    "Tools can also be defined that have access to the workflow context. This means you can set and retrieve variables from the context and use them in the tool or between tools.\n",
    "\n",
    "`AgentWorkflow` uses a context variable called `state` that gets passed to every agent. You can rely on information in `state` being available without explicitly having to pass it in.\n",
    "\n",
    "**Note:** To access the `Context`, the Context parameter should be the first parameter of the tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0656c35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your name has been set to Daniil.\n",
      "Name as stored in state:  Daniil\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.workflow import Context\n",
    "\n",
    "async def set_name(ctx: Context, name: str) -> str:\n",
    "    state = await ctx.get(\"state\")\n",
    "    state[\"name\"] = name\n",
    "    await ctx.set(\"state\", state)\n",
    "    return f\"Name set to {name}\"\n",
    "\n",
    "\n",
    "workflow = AgentWorkflow.from_tools_or_functions(\n",
    "    [set_name],\n",
    "    llm=llm,\n",
    "    system_prompt=\"You are a helpful assistant that can set a name.\",\n",
    "    initial_state={\"name\": \"unset\"},\n",
    ")\n",
    "\n",
    "ctx = Context(workflow)\n",
    "\n",
    "response = await workflow.run(user_msg=\"My name is Daniil\", ctx=ctx)\n",
    "print(str(response))\n",
    "\n",
    "state = await ctx.get(\"state\")\n",
    "print(\"Name as stored in state: \",state[\"name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be812f7",
   "metadata": {},
   "source": [
    "## Human in the Loop\n",
    "\n",
    "Tools can also be defined that involve a human in the loop. This is useful for tasks that require human input, such as confirming a tool call or providing feedback.\n",
    "\n",
    "As we'll see in the next section, the way `Workflow`s work under the hood of `AgentWorkflow` is by running `step`s which both emit and receive events. Here's a diagram of the steps (in blue) that makes up an AgentWorkflow and the events (in green) that pass data between them. You'll recognize these events, they're the same ones we were handling in the output stream earlier.\n",
    "\n",
    " To get a human in the loop, we'll get our tool to emit an event that isn't received by any other step in the workflow. We'll then tell our tool to wait until it receives a specific \"reply\" event.\n",
    "\n",
    "This is a very common pattern for human in the loop, so we have built-in `InputRequiredEvent` and `HumanResponseEvent` events to use for this purpose. If you want to capture different forms of human input, you can subclass these events to match your own preferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01c28a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import (\n",
    "    Context,\n",
    "    InputRequiredEvent,\n",
    "    HumanResponseEvent,\n",
    ")\n",
    "\n",
    "# a tool that performs a dangerous task\n",
    "async def dangerous_task(ctx: Context) -> str:\n",
    "    \"\"\"A dangerous task that requires human confirmation.\"\"\"\n",
    "\n",
    "    # emit an event to the external stream to be captured\n",
    "    ctx.write_event_to_stream(\n",
    "        InputRequiredEvent(\n",
    "            prefix=\"Are you sure you want to proceed? \",\n",
    "            user_name=\"Daniil\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # wait until we see a HumanResponseEvent\n",
    "    response = await ctx.wait_for_event(\n",
    "        HumanResponseEvent, requirements={\"user_name\": \"Daniil\"}\n",
    "    )\n",
    "\n",
    "    # act on the input from the event\n",
    "    if response.response.strip().lower() == \"yes\":\n",
    "        return \"Dangerous task completed successfully.\"\n",
    "    else:\n",
    "        return \"Dangerous task aborted.\"\n",
    "\n",
    "\n",
    "workflow = AgentWorkflow.from_tools_or_functions(\n",
    "    [dangerous_task],\n",
    "    llm=llm,\n",
    "    system_prompt=\"You are a helpful assistant that can perform dangerous tasks.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c379f6",
   "metadata": {},
   "source": [
    "To capture the event, we use the same streaming interface we used earlier and look for an `InputRequiredEvent`. Then we can use `input` to capture a response from the user, and send it back using the `send_event` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b1dbc21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dangerous task has been aborted. If you wish to proceed, please confirm that you are ready to continue with the task.\n"
     ]
    }
   ],
   "source": [
    "handler = workflow.run(user_msg=\"I want to proceed with the dangerous task.\")\n",
    "\n",
    "async for event in handler.stream_events():\n",
    "    # capture InputRequiredEvent\n",
    "    if isinstance(event, InputRequiredEvent):\n",
    "        # capture keyboard input\n",
    "        response = input(event.prefix)\n",
    "        # send our response back\n",
    "        handler.ctx.send_event(\n",
    "            HumanResponseEvent(\n",
    "                response=response,\n",
    "                user_name=event.user_name,\n",
    "            )\n",
    "        )\n",
    "\n",
    "response = await handler\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc03d7d5",
   "metadata": {},
   "source": [
    "## Putting together a multi-agent system\n",
    "\n",
    "Now we've seen all the ways workflows can work, let's construct our own multi-agent system!\n",
    "\n",
    "We'll have to create new agents, since the FunctionCallingAgents we made earlier are specifically adapted to run inside of `AgentWorkflow`. Making a generic function calling agent is very similar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54361919",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FunctionCallingAgent as GenericFunctionCallingAgent\n",
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "# convert our web search functions into a tool, the generic function calling agent doesn't do this automatically (yet)\n",
    "search_web_tool = FunctionTool.from_defaults(fn=search_web)\n",
    "\n",
    "research_agent = GenericFunctionCallingAgent.from_tools(\n",
    "    tools=[search_web_tool],\n",
    "    llm=llm,\n",
    "    verbose=False,\n",
    "    allow_parallel_tool_calls=False,\n",
    "    system_prompt=\"You are an agent that does research by searching the web and then records the results of your research.\"\n",
    ")\n",
    "write_agent = GenericFunctionCallingAgent.from_tools(\n",
    "    tools=[],\n",
    "    llm=llm,\n",
    "    verbose=False,\n",
    "    allow_parallel_tool_calls=False,\n",
    "    system_prompt=\"You are an agent that writes a report based on the results of research by another agent.\"\n",
    ")\n",
    "review_agent = GenericFunctionCallingAgent.from_tools(\n",
    "    tools=[],\n",
    "    llm=llm,\n",
    "    verbose=False,\n",
    "    allow_parallel_tool_calls=False,\n",
    "    system_prompt=\"You are an agent that reviews a report written by a different agent.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdf3e49",
   "metadata": {},
   "source": [
    "We'll create a simple linear flow to start with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e19bc6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import Event, Workflow, Context, StopEvent, step\n",
    "from llama_index.core.workflow import StartEvent\n",
    "\n",
    "class ResearchEvent(Event):\n",
    "    prompt: str\n",
    "\n",
    "class WriteEvent(Event):\n",
    "    research: str\n",
    "\n",
    "class ReviewEvent(Event):\n",
    "    report: str\n",
    "\n",
    "class ReviewResults(Event):\n",
    "    review: str\n",
    "\n",
    "class SimpleMultiAgentFlow(Workflow):\n",
    "\n",
    "    @step\n",
    "    async def setup(self, ev: StartEvent) -> ResearchEvent:\n",
    "        self.research_agent = ev.research_agent\n",
    "        self.write_agent = ev.write_agent\n",
    "        self.review_agent = ev.review_agent\n",
    "        return ResearchEvent(prompt=ev.prompt)\n",
    "\n",
    "    @step\n",
    "    async def research(self, ctx: Context, ev: ResearchEvent) -> WriteEvent:\n",
    "\n",
    "        await ctx.set(\"prompt\", ev.prompt)\n",
    "        result = self.research_agent.chat(f\"Do some research that another agent will use to write a report about this topic: <topic>{ev.prompt}</topic>. Just include the facts without making it into a full report.\")\n",
    "\n",
    "        return WriteEvent(research=str(result))\n",
    "\n",
    "    @step\n",
    "    async def write(self, ctx: Context, ev: WriteEvent) -> ReviewEvent:\n",
    "        result = self.write_agent.chat(f\"Write a report based on this research: <research>{ev.research}</research> and this topic: <topic>{await ctx.get('prompt')}</topic>\")\n",
    "\n",
    "        return ReviewEvent(report=str(result))\n",
    "\n",
    "    @step\n",
    "    async def review(self, ctx: Context, ev: ReviewEvent) -> StopEvent:\n",
    "        result = self.review_agent.chat(f\"Review this report: <report>{ev.report}</report>\")\n",
    "\n",
    "        ctx.write_event_to_stream(ReviewResults(review=str(result)))\n",
    "\n",
    "        return StopEvent(result=ev.report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdcbdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "workflow = SimpleMultiAgentFlow(timeout=30, verbose=True)\n",
    "handler = workflow.run(\n",
    "    prompt=\"History of San Francisco\",\n",
    "    research_agent=research_agent,\n",
    "    write_agent=write_agent,\n",
    "    review_agent=review_agent\n",
    ")\n",
    "\n",
    "async for ev in handler.stream_events():\n",
    "    if isinstance(ev, ReviewResults):\n",
    "        print(\"==== The review ====\")\n",
    "        print(ev.review)\n",
    "\n",
    "final_result = await handler\n",
    "print(\"==== The report ====\")\n",
    "print(final_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5718af8",
   "metadata": {},
   "source": [
    "## Adding Reflection\n",
    "\n",
    "But we mentioned reflection as an extra feature we can add because we have fine-grained control. Let's add that!\n",
    "\n",
    "We need to make several changes:\n",
    "* In `research` we'll store the research into the context, since we might need to use it multiple times\n",
    "* We'll tell `write` that it can be triggered by a `RewriteEvent` in addition to a `WriteEvent`\n",
    "* If it's a `RewriteEvent` we'll add the review as feedback to the prompt\n",
    "* `review` will be changed to optionally emit a `RewriteEvent`\n",
    "* We'll get the LLM to decide if the review returned by the agent is a \"bad\" or \"good\" review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e23b2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewriteEvent(Event):\n",
    "    review: str\n",
    "\n",
    "# Our WriteEvent doesn't need research attached any more\n",
    "class WriteEvent(Event):\n",
    "    pass\n",
    "\n",
    "class MultiAgentFlowWithReflection(Workflow):\n",
    "\n",
    "    @step\n",
    "    async def setup(self, ev: StartEvent) -> ResearchEvent:\n",
    "        self.research_agent = ev.research_agent\n",
    "        self.write_agent = ev.write_agent\n",
    "        self.review_agent = ev.review_agent\n",
    "        return ResearchEvent(prompt=ev.prompt)\n",
    "\n",
    "    @step\n",
    "    async def research(self, ctx: Context, ev: ResearchEvent) -> WriteEvent:\n",
    "\n",
    "        await ctx.set(\"prompt\", ev.prompt)\n",
    "        result = self.research_agent.chat(f\"Do some research that another agent will use to write a report about this topic: <topic>{ev.prompt}</topic>. Just include the facts without making it into a full report.\")\n",
    "\n",
    "        # store the research in the context for multiple uses\n",
    "        await ctx.set(\"research\", str(result))\n",
    "\n",
    "        return WriteEvent()\n",
    "\n",
    "    @step\n",
    "    async def write(self, ctx: Context, ev: WriteEvent | RewriteEvent ) -> ReviewEvent:\n",
    "\n",
    "        prompt = f\"Write a report based on this research: <research>{await ctx.get('research')}</research> and this topic: <topic>{await ctx.get('prompt')}</topic> \"\n",
    "\n",
    "        # detect RewriteEvents and modify the prompt\n",
    "        if isinstance(ev, RewriteEvent):\n",
    "            print(\"Doing a rewrite!\")\n",
    "            prompt += f\"This report has reviewed and the reviewer had this feedback, which you should take into account: <review>{ev.review}</review>\"\n",
    "\n",
    "        result = self.write_agent.chat(prompt)\n",
    "\n",
    "        return ReviewEvent(report=str(result))\n",
    "\n",
    "    @step\n",
    "    async def review(self, ctx: Context, ev: ReviewEvent) -> StopEvent | RewriteEvent:\n",
    "        result = self.review_agent.chat(f\"Review this report: {ev.report}\")\n",
    "\n",
    "        # get the LLM to self-reflect\n",
    "        try_again = llm.complete(f\"This is a review of a report written by an agent. If you think this review is bad enough that the agent should try again, respond with just the word RETRY. If the review is good, reply with just the word CONTINUE. Here's the review: <review>{str(result)}</review>\")\n",
    "\n",
    "        if try_again.text == \"RETRY\":\n",
    "            print(\"Reviewer said try again\")\n",
    "            return RewriteEvent(review=str(result))\n",
    "        else:\n",
    "            print(\"Reviewer thought it was good!\")\n",
    "            return StopEvent(result=ev.report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17133fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = MultiAgentFlowWithReflection(timeout=30, verbose=True)\n",
    "handler = workflow.run(\n",
    "    prompt=\"History Smolensk, Russia\",\n",
    "    research_agent=research_agent,\n",
    "    write_agent=write_agent,\n",
    "    review_agent=review_agent\n",
    ")\n",
    "\n",
    "final_result = await handler\n",
    "print(\"==== The report ====\")\n",
    "print(final_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
