{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f4a2c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install llama-index llama-index-embeddings-openai qdrant-client llama-index-vector-stores-qdrant llama-index llama-index-llms-openai llama-index-vector-stores-faiss faiss-cpu llama-index-llms-anthropic tavily-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e8396ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nest_asyncio\n",
    "from getpass import getpass\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(r\"C:\\Users\\anteb\\PycharmProjects\\JupyterProject\\.env\")\n",
    "nest_asyncio.apply()\n",
    "\n",
    "CO_API_KEY = os.environ.get('CO_API_KEY') or getpass(\"Enter CO_API_KEY: \")\n",
    "OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY') or getpass(\"Enter OPENAI_API_KEY: \")\n",
    "QDRANT_URL = os.environ.get('QDRANT_URL') or getpass(\"Enter QDRANT_URL: \")\n",
    "QDRANT_API_KEY = os.environ.get('QDRANT_API_KEY') or getpass(\"Enter QDRANT_API_KEY: \")\n",
    "TAVILY_API_KEY = os.environ.get('TAVILY_API_KEY') or getpass(\"Enter TAVILY_API_KEY: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ce477171",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core import Settings\n",
    "\n",
    "llm = OpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=0.5, max_tokens=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a200c9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tavily import AsyncTavilyClient\n",
    "\n",
    "# note the type annotations for the incoming query and the return string\n",
    "async def search_web(query: str) -> str:\n",
    "    \"\"\"Useful for using the web to answer questions.\"\"\"\n",
    "    client = AsyncTavilyClient(api_key=TAVILY_API_KEY)\n",
    "    return str(await client.search(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3aa6abc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent.workflow import AgentWorkflow\n",
    "\n",
    "workflow = AgentWorkflow.from_tools_or_functions(\n",
    "    [search_web],\n",
    "    system_prompt=\"You are a helpful assistant that answers questions. If you don't know the answer, you can search the web for information.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a1a014b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current weather in Smolensk, Russia, is as follows:\n",
      "\n",
      "- **Temperature**: 7.7°C (45.9°F)\n",
      "- **Condition**: Sunny\n",
      "- **Wind**: 4.3 mph (6.8 kph) from the northwest\n",
      "- **Humidity**: 70%\n",
      "- **Visibility**: 10 km\n",
      "- **Pressure**: 1017 mb\n",
      "\n",
      "For more details, you can check the weather on [WeatherAPI](https://www.weatherapi.com/) or [AccuWeather](https://www.accuweather.com/en/ru/smolensk/295475/weather-forecast/295475).\n"
     ]
    }
   ],
   "source": [
    "response = await workflow.run(user_msg=\"What is the weather in Smolensk Russia?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db2a1fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice to meet you, Daniil! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.workflow import Context\n",
    "\n",
    "# configure a context to work with our workflow\n",
    "ctx = Context(workflow)\n",
    "\n",
    "response = await workflow.run(\n",
    "    user_msg=\"My name is Daniil, nice to meet you!\", ctx=ctx # give the configured context to the workflow\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1059c7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import JsonPickleSerializer, JsonSerializer\n",
    "\n",
    "# convert our Context to a dictionary object\n",
    "ctx_dict = ctx.to_dict(serializer=JsonSerializer())\n",
    "\n",
    "# create a new Context from the dictionary\n",
    "restored_ctx = Context.from_dict(\n",
    "    workflow, ctx_dict, serializer=JsonSerializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e84272cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, I remember your name, Daniil! How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "response = await workflow.run(\n",
    "    user_msg=\"Do you still remember my name?\", ctx=restored_ctx\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9051ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent.workflow import (\n",
    "    AgentInput,\n",
    "    AgentOutput,\n",
    "    ToolCall,\n",
    "    ToolCallResult,\n",
    "    AgentStream,\n",
    ")\n",
    "\n",
    "handler = workflow.run(user_msg=\"What is the weather in Smolenks? Call me by name when you anser\", ctx=restored_ctx)\n",
    "\n",
    "\n",
    "async for event in handler.stream_events():\n",
    "    if isinstance(event, AgentInput):\n",
    "       print(\"Agent input: \", event.input)  # the current input messages\n",
    "       print(\"Agent name:\", event.current_agent_name)  # the current agent name\n",
    "    elif isinstance(event, AgentOutput):\n",
    "       print(\"Agent output: \", event.response)  # the current full response\n",
    "       print(\"Tool calls made: \", event.tool_calls)  # the selected tool calls, if any\n",
    "       print(\"Raw LLM response: \", event.raw)  # the raw llm api response\n",
    "    elif isinstance(event, ToolCallResult):\n",
    "       print(\"Tool called: \", event.tool_name)  # the tool name\n",
    "       print(\"Arguments to the tool: \", event.tool_kwargs)  # the tool kwargs\n",
    "       print(\"Tool output: \", event.tool_output)  # the tool output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e42961d",
   "metadata": {},
   "source": [
    "## Tools and State\n",
    "\n",
    "Tools can also be defined that have access to the workflow context. This means you can set and retrieve variables from the context and use them in the tool or between tools.\n",
    "\n",
    "`AgentWorkflow` uses a context variable called `state` that gets passed to every agent. You can rely on information in `state` being available without explicitly having to pass it in.\n",
    "\n",
    "**Note:** To access the `Context`, the Context parameter should be the first parameter of the tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0656c35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your name has been set to Daniil.\n",
      "Name as stored in state:  Daniil\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.workflow import Context\n",
    "\n",
    "async def set_name(ctx: Context, name: str) -> str:\n",
    "    state = await ctx.get(\"state\")\n",
    "    state[\"name\"] = name\n",
    "    await ctx.set(\"state\", state)\n",
    "    return f\"Name set to {name}\"\n",
    "\n",
    "\n",
    "workflow = AgentWorkflow.from_tools_or_functions(\n",
    "    [set_name],\n",
    "    llm=llm,\n",
    "    system_prompt=\"You are a helpful assistant that can set a name.\",\n",
    "    initial_state={\"name\": \"unset\"},\n",
    ")\n",
    "\n",
    "ctx = Context(workflow)\n",
    "\n",
    "response = await workflow.run(user_msg=\"My name is Daniil\", ctx=ctx)\n",
    "print(str(response))\n",
    "\n",
    "state = await ctx.get(\"state\")\n",
    "print(\"Name as stored in state: \",state[\"name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be812f7",
   "metadata": {},
   "source": [
    "## Human in the Loop\n",
    "\n",
    "Tools can also be defined that involve a human in the loop. This is useful for tasks that require human input, such as confirming a tool call or providing feedback.\n",
    "\n",
    "As we'll see in the next section, the way `Workflow`s work under the hood of `AgentWorkflow` is by running `step`s which both emit and receive events. Here's a diagram of the steps (in blue) that makes up an AgentWorkflow and the events (in green) that pass data between them. You'll recognize these events, they're the same ones we were handling in the output stream earlier.\n",
    "\n",
    " To get a human in the loop, we'll get our tool to emit an event that isn't received by any other step in the workflow. We'll then tell our tool to wait until it receives a specific \"reply\" event.\n",
    "\n",
    "This is a very common pattern for human in the loop, so we have built-in `InputRequiredEvent` and `HumanResponseEvent` events to use for this purpose. If you want to capture different forms of human input, you can subclass these events to match your own preferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "01c28a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import (\n",
    "    Context,\n",
    "    InputRequiredEvent,\n",
    "    HumanResponseEvent,\n",
    ")\n",
    "\n",
    "# a tool that performs a dangerous task\n",
    "async def dangerous_task(ctx: Context) -> str:\n",
    "    \"\"\"A dangerous task that requires human confirmation.\"\"\"\n",
    "\n",
    "    # emit an event to the external stream to be captured\n",
    "    ctx.write_event_to_stream(\n",
    "        InputRequiredEvent(\n",
    "            prefix=\"Are you sure you want to proceed? \",\n",
    "            user_name=\"Daniil\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # wait until we see a HumanResponseEvent\n",
    "    response = await ctx.wait_for_event(\n",
    "        HumanResponseEvent, requirements={\"user_name\": \"Daniil\"}\n",
    "    )\n",
    "\n",
    "    # act on the input from the event\n",
    "    if response.response.strip().lower() == \"yes\":\n",
    "        return \"Dangerous task completed successfully.\"\n",
    "    else:\n",
    "        return \"Dangerous task aborted.\"\n",
    "\n",
    "\n",
    "workflow = AgentWorkflow.from_tools_or_functions(\n",
    "    [dangerous_task],\n",
    "    llm=llm,\n",
    "    system_prompt=\"You are a helpful assistant that can perform dangerous tasks.\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
